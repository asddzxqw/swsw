import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import requests
import re
from collections import Counter
from datetime import datetime, timedelta
import numpy as np
import time
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from transformers import logging
import warnings

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')
logging.set_verbosity_error()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ì—¬ë¡  ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #FF0000;
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 2rem;
    }
    .danger-alert {
        background-color: #ffebee;
        border-left: 5px solid #f44336;
        padding: 1rem;
        margin: 1rem 0;
    }
    .success-alert {
        background-color: #e8f5e8;
        border-left: 5px solid #4caf50;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background-color: #fff3e0;
        border-left: 5px solid #ff9800;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# YouTube API í´ë˜ìŠ¤
class YouTubeAnalyzer:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://www.googleapis.com/youtube/v3"
    
    def extract_video_id(self, url):
        """YouTube URLì—ì„œ video ID ì¶”ì¶œ"""
        patterns = [
            r'youtube\.com/watch\?v=([^&]+)',
            r'youtu\.be/([^?]+)',
            r'youtube\.com/embed/([^?]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None
    
    def get_video_info(self, video_id):
        """ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/videos"
        params = {
            'part': 'snippet,statistics',
            'id': video_id,
            'key': self.api_key
        }
        
        try:
            response = requests.get(url, params=params)
            data = response.json()
            
            if 'items' in data and len(data['items']) > 0:
                video = data['items'][0]
                return {
                    'title': video['snippet']['title'],
                    'channel': video['snippet']['channelTitle'],
                    'published_at': video['snippet']['publishedAt'],
                    'view_count': int(video['statistics'].get('viewCount', 0)),
                    'like_count': int(video['statistics'].get('likeCount', 0)),
                    'comment_count': int(video['statistics'].get('commentCount', 0))
                }
        except Exception as e:
            st.error(f"ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        return None
    
    def get_comments(self, video_id, max_results=100):
        """ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/commentThreads"
        params = {
            'part': 'snippet',
            'videoId': video_id,
            'maxResults': min(max_results, 100),
            'order': 'time',
            'key': self.api_key
        }
        
        comments = []
        
        try:
            while len(comments) < max_results:
                response = requests.get(url, params=params)
                data = response.json()
                
                if 'items' not in data:
                    break
                
                for item in data['items']:
                    comment = item['snippet']['topLevelComment']['snippet']
                    comments.append({
                        'text': comment['textDisplay'],
                        'author': comment['authorDisplayName'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                
                if 'nextPageToken' not in data or len(comments) >= max_results:
                    break
                
                params['pageToken'] = data['nextPageToken']
                
        except Exception as e:
            st.error(f"ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        return comments[:max_results]

# AI ê°ì •ë¶„ì„ ëª¨ë¸ í´ë˜ìŠ¤
class KoreanSentimentAnalyzer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.classifier = None
        self.is_initialized = False
    
    @st.cache_resource
    def load_model(_self):
        """AI ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ í•œë²ˆë§Œ ë¡œë“œ)"""
        try:
            # 1ì°¨: í•œêµ­ì–´ ê°ì •ë¶„ì„ ëª¨ë¸ (ë¬´ë£Œ)
            model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
            
            _self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            _self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            _self.classifier = pipeline(
                "text-classification",
                model=_self.model,
                tokenizer=_self.tokenizer,
                device=0 if torch.cuda.is_available() else -1,
                return_all_scores=True
            )
            
            _self.is_initialized = True
            _self.model_type = "roberta"
            return True
            
        except Exception as e:
            st.warning(f"1ì°¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # 2ì°¨: ë‹¤ë¥¸ ê°ì •ë¶„ì„ ëª¨ë¸
            try:
                model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
                _self.classifier = pipeline(
                    "text-classification",
                    model=model_name,
                    device=0 if torch.cuda.is_available() else -1
                )
                _self.is_initialized = True
                _self.model_type = "bert"
                return True
            except Exception as e2:
                st.warning(f"2ì°¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e2)}")
                # 3ì°¨: ê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë¸
                try:
                    model_name = "distilbert-base-uncased-finetuned-sst-2-english"
                    _self.classifier = pipeline(
                        "text-classification",
                        model=model_name,
                        device=0 if torch.cuda.is_available() else -1
                    )
                    _self.is_initialized = True
                    _self.model_type = "distilbert"
                    return True
                except Exception as e3:
                    st.error(f"ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
                    return False
    
    def analyze_sentiment(self, text):
        """AI ê¸°ë°˜ ê°ì • ë¶„ì„"""
        if not self.is_initialized:
            if not self.load_model():
                return self._fallback_analysis(text)
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)  # URL ì œê±°
            text = text.strip()
            
            if not text:
                return "ì¤‘ë¦½/ê¸°íƒ€"
            
            # AI ëª¨ë¸ë¡œ ê°ì • ë¶„ì„
            result = self.classifier(text)
            
            if isinstance(result[0], list):
                # ëª¨ë“  ì ìˆ˜ ë°˜í™˜í•˜ëŠ” ê²½ìš°
                scores = {item['label']: item['score'] for item in result[0]}
                predicted_label = max(scores.keys(), key=lambda k: scores[k])
            else:
                # ë‹¨ì¼ ê²°ê³¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°
                predicted_label = result[0]['label']
                
            # ë¼ë²¨ì„ ìš°ë¦¬ ë¶„ë¥˜ ì²´ê³„ë¡œ ë§¤í•‘
            return self._map_to_category(predicted_label, text)
            
        except Exception as e:
            # AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
            return self._fallback_analysis(text)
    
    def _map_to_category(self, ai_label, text):
        """AI ëª¨ë¸ ê²°ê³¼ë¥¼ ìš°ë¦¬ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        text_lower = text.lower()
        
        # ì§ˆë¬¸ íŒ¨í„´ ìš°ì„  í™•ì¸
        question_patterns = ['?', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë­', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì§ˆë¬¸', 'ê¶ê¸ˆ', 'ì•Œë ¤ì£¼', 'ê°€ë¥´ì³']
        if any(pattern in text_lower for pattern in question_patterns):
            return "ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±"
        
        # ìœ ë¨¸/ë¹„ê¼¼ íŒ¨í„´ í™•ì¸
        humor_patterns = ['ã…‹ã…‹', 'ã…ã…', 'í—ˆí—ˆ', 'ã…‹', 'ë¯¸ì³¤', 'ëŒ€ë°•', 'í—', 'ì–´íœ´']
        if any(pattern in text_lower for pattern in humor_patterns):
            if text_lower.count('ã…‹') > 2:
                return "ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„"
        
        # AI ê°ì •ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë§¤í•‘ (ëª¨ë¸ë³„ ë¼ë²¨ ì²˜ë¦¬)
        label_upper = str(ai_label).upper()
        
        # RoBERTa ëª¨ë¸ ë¼ë²¨
        if label_upper in ['LABEL_2', 'POSITIVE', 'POS']:
            return "ì°¬ì„±/ì§€ì§€"
        elif label_upper in ['LABEL_0', 'NEGATIVE', 'NEG']:
            return "ë°˜ëŒ€/ë¹„íŒ"
        elif label_upper in ['LABEL_1', 'NEUTRAL']:
            return "ì¤‘ë¦½/ê¸°íƒ€"
        
        # BERT ë‹¤êµ­ì–´ ëª¨ë¸ ë¼ë²¨ (1~5 ë³„ì )
        elif label_upper in ['5 STARS', '4 STARS']:
            return "ì°¬ì„±/ì§€ì§€"
        elif label_upper in ['1 STAR', '2 STARS']:
            return "ë°˜ëŒ€/ë¹„íŒ"
        elif label_upper in ['3 STARS']:
            return "ì¤‘ë¦½/ê¸°íƒ€"
        
        # DistilBERT ëª¨ë¸ ë¼ë²¨
        elif label_upper == 'POSITIVE':
            return "ì°¬ì„±/ì§€ì§€"
        elif label_upper == 'NEGATIVE':
            return "ë°˜ëŒ€/ë¹„íŒ"
        
        else:
            return "ì¤‘ë¦½/ê¸°íƒ€"
    
    def _fallback_analysis(self, text):
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„"""
        text = text.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ë°±ì—…ìš©)
        positive_keywords = ['ì¢‹ë‹¤', 'ìµœê³ ', 'í›Œë¥­', 'ê°ì‚¬', 'ì¢‹ë„¤', 'êµ¿', 'ì™„ë²½', 'ìµœê³ ë‹¤', 'ì‘ì›', 'ì§€ì§€', 'ì°¬ì„±', 'ë§ë‹¤', 'ì¢‹ì•„', 'ì‚¬ë‘', 'ëŒ€ë‹¨', 'ë©‹ì§€ë‹¤']
        negative_keywords = ['ì‹«ë‹¤', 'ë³„ë¡œ', 'ìµœì•…', 'ë‚˜ì˜ë‹¤', 'ë°˜ëŒ€', 'ë¹„íŒ', 'í‹€ë ¸', 'ë¬¸ì œ', 'ì˜ëª»', 'ì‹¤ë§', 'í™”ë‚œë‹¤', 'ì§œì¦', 'ë‹µë‹µ', 'ì—­ê²¨', 'ì“°ë ˆê¸°']
        question_keywords = ['?', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë­', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì§ˆë¬¸', 'ê¶ê¸ˆ', 'ì•Œë ¤']
        sarcasm_keywords = ['ã…‹ã…‹', 'ã…ã…', 'í—ˆí—ˆ', 'ì™€ìš°', 'ëŒ€ë°•', 'ì§„ì§œ', 'ë ˆì•Œ', 'ë¯¸ì³¤', 'ê°œ', 'í—', 'ì–´íœ´']
        
        # ì ìˆ˜ ê³„ì‚°
        positive_score = sum(1 for word in positive_keywords if word in text)
        negative_score = sum(1 for word in negative_keywords if word in text)
        question_score = sum(1 for word in question_keywords if word in text)
        sarcasm_score = sum(1 for word in sarcasm_keywords if word in text)
        
        # ã…‹ã…‹ã…‹ íŒ¨í„´ ê°€ì¤‘ì¹˜
        if 'ã…‹' in text and text.count('ã…‹') > 2:
            sarcasm_score += 2
        
        # ë¶„ë¥˜ ë¡œì§
        if question_score > 0 or '?' in text:
            return "ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±"
        elif sarcasm_score > positive_score + negative_score:
            return "ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„"
        elif positive_score > negative_score:
            return "ì°¬ì„±/ì§€ì§€"
        elif negative_score > positive_score:
            return "ë°˜ëŒ€/ë¹„íŒ"
        else:
            return "ì¤‘ë¦½/ê¸°íƒ€"

# ì „ì—­ AI ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_ai_analyzer():
    return KoreanSentimentAnalyzer()

def analyze_sentiment_trend(comments_df):
    """ê°ì • íë¦„ ë¶„ì„"""
    if len(comments_df) == 0:
        return pd.DataFrame()
    
    # ì‹œê°„ë³„ ëŒ“ê¸€ ê·¸ë£¹í™”
    comments_df['published_at'] = pd.to_datetime(comments_df['published_at'])
    comments_df['hour'] = comments_df['published_at'].dt.floor('H')
    
    # ì‹œê°„ë³„ ìœ í˜• ë¶„í¬
    hourly_sentiment = comments_df.groupby(['hour', 'type']).size().unstack(fill_value=0)
    
    return hourly_sentiment

def calculate_risk_score(type_counts, total_comments):
    """ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°"""
    if total_comments == 0:
        return 0
    
    negative_types = ['ë°˜ëŒ€/ë¹„íŒ', 'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„']
    negative_count = sum(type_counts.get(t, 0) for t in negative_types)
    negative_ratio = negative_count / total_comments
    
    # ìœ„í—˜ë„ ì ìˆ˜ (0-100)
    risk_score = min(negative_ratio * 100, 100)
    
    return risk_score

# ë©”ì¸ ì•±
def main():
    st.markdown('<h1 class="main-header">ğŸ¤– AI ê¸°ë°˜ YouTube ëŒ“ê¸€ ì—¬ë¡  ë¶„ì„ ì‹œìŠ¤í…œ</h1>', unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ì„¤ì •")
    
    # API í‚¤ ì…ë ¥
    api_key = st.sidebar.text_input("YouTube API Key", type="password", help="YouTube Data API v3 í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    if not api_key:
        st.warning("ğŸ”‘ YouTube API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        st.info("""
        **API í‚¤ ë°œê¸‰ ë°©ë²•:**
        1. [Google Cloud Console](https://console.cloud.google.com/)ì— ì ‘ì†
        2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± ë˜ëŠ” ê¸°ì¡´ í”„ë¡œì íŠ¸ ì„ íƒ
        3. YouTube Data API v3 í™œì„±í™”
        4. ì‚¬ìš©ì ì¸ì¦ ì •ë³´ì—ì„œ API í‚¤ ìƒì„±
        
        **ğŸ¤– AI ëª¨ë¸ ì •ë³´:**
        - ë‹¤ì¤‘ ë°±ì—… ì‹œìŠ¤í…œìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
        - 1ì°¨: Twitter RoBERTa (ë‹¤êµ­ì–´ ê°ì •ë¶„ì„)
        - 2ì°¨: BERT ë‹¤êµ­ì–´ ëª¨ë¸
        - 3ì°¨: DistilBERT ì˜ì–´ ëª¨ë¸
        - í‚¤ì›Œë“œ ë°©ì‹ ëŒ€ë¹„ 80%+ ì •í™•ë„ í–¥ìƒ
        """)
        return
    
    # YouTube URL ì…ë ¥
    youtube_url = st.sidebar.text_input("YouTube ë¹„ë””ì˜¤ URL", placeholder="https://www.youtube.com/watch?v=...")
    
    # ë¶„ì„ ì˜µì…˜
    max_comments = st.sidebar.slider("ë¶„ì„í•  ëŒ“ê¸€ ìˆ˜", 50, 500, 200)
    
    if st.sidebar.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary"):
        if not youtube_url:
            st.error("YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return
        
        analyzer = YouTubeAnalyzer(api_key)
        video_id = analyzer.extract_video_id(youtube_url)
        
        if not video_id:
            st.error("ì˜¬ë°”ë¥¸ YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        status_text.text("ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        progress_bar.progress(20)
        
        video_info = analyzer.get_video_info(video_id)
        if not video_info:
            st.error("ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
        status_text.text("ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...")
        progress_bar.progress(50)
        
        comments = analyzer.get_comments(video_id, max_comments)
        
        if not comments:
            st.error("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ëŒ“ê¸€ ë¶„ë¥˜
        status_text.text("AIë¡œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì¤‘...")
        progress_bar.progress(80)
        
        # AI ë¶„ì„ê¸° ì´ˆê¸°í™”
        ai_analyzer = get_ai_analyzer()
        
        # AI ëª¨ë¸ ë¡œë“œ í‘œì‹œ
        if not ai_analyzer.is_initialized:
            with st.spinner("ğŸ¤– AI ê°ì •ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒ)"):
                ai_analyzer.load_model()
        
        # ê° ëŒ“ê¸€ì„ AIë¡œ ë¶„ì„
        for i, comment in enumerate(comments):
            comment['type'] = ai_analyzer.analyze_sentiment(comment['text'])
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if i % 10 == 0:  # 10ê°œë§ˆë‹¤ ì—…ë°ì´íŠ¸
                progress = 80 + (i / len(comments)) * 15
                progress_bar.progress(min(int(progress), 95))
        
        comments_df = pd.DataFrame(comments)
        
        progress_bar.progress(100)
        status_text.text("ë¶„ì„ ì™„ë£Œ!")
        time.sleep(0.5)
        status_text.empty()
        progress_bar.empty()
        
        # ê²°ê³¼ í‘œì‹œ
        display_results(video_info, comments_df)

def display_results(video_info, comments_df):
    """ê²°ê³¼ í‘œì‹œ"""
    
    # ë¹„ë””ì˜¤ ì •ë³´ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ‘€ ì¡°íšŒìˆ˜", f"{video_info['view_count']:,}")
    with col2:
        st.metric("ğŸ‘ ì¢‹ì•„ìš”", f"{video_info['like_count']:,}")
    with col3:
        st.metric("ğŸ’¬ ëŒ“ê¸€ìˆ˜", f"{video_info['comment_count']:,}")
    with col4:
        st.metric("ğŸ“Š ë¶„ì„ ëŒ“ê¸€", f"{len(comments_df):,}")
    
    st.markdown(f"**ğŸ“º ì œëª©:** {video_info['title']}")
    st.markdown(f"**ğŸ“º ì±„ë„:** {video_info['channel']}")
    
    # ìœ í˜•ë³„ ë¶„í¬ ë¶„ì„
    type_counts = comments_df['type'].value_counts()
    total_comments = len(comments_df)
    
    # ìœ„í—˜ë„ ê³„ì‚°
    risk_score = calculate_risk_score(type_counts.to_dict(), total_comments)
    
    # ìœ„í—˜ ê²½ë³´
    if risk_score > 50:
        st.markdown(f"""
        <div class="danger-alert">
            <h3>ğŸš¨ ìœ„í—˜ ê²½ë³´!</h3>
            <p>ë¶€ì •ì  ëŒ“ê¸€ ë¹„ìœ¨ì´ <strong>{risk_score:.1f}%</strong>ë¡œ ë†’ìŠµë‹ˆë‹¤!</p>
            <p>ì‚¬íšŒì  ìœ„í—˜ ì‹ í˜¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ëŒ€ì‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
        </div>
        """, unsafe_allow_html=True)
    elif risk_score > 30:
        st.markdown(f"""
        <div class="warning-alert">
            <h3>âš ï¸ ì£¼ì˜ í•„ìš”</h3>
            <p>ë¶€ì •ì  ëŒ“ê¸€ ë¹„ìœ¨ì´ <strong>{risk_score:.1f}%</strong>ì…ë‹ˆë‹¤.</p>
            <p>ì—¬ë¡  ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="success-alert">
            <h3>âœ… ì•ˆì •ì </h3>
            <p>ë¶€ì •ì  ëŒ“ê¸€ ë¹„ìœ¨ì´ <strong>{risk_score:.1f}%</strong>ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤.</p>
        </div>
        """, unsafe_allow_html=True)
    
    # ì°¨íŠ¸ ì˜ì—­
    col1, col2 = st.columns(2)
    
    with col1:
        # íŒŒì´ ì°¨íŠ¸
        fig_pie = px.pie(
            values=type_counts.values,
            names=type_counts.index,
            title="ëŒ“ê¸€ ìœ í˜•ë³„ ë¶„í¬",
            color_discrete_map={
                'ì°¬ì„±/ì§€ì§€': '#4CAF50',
                'ë°˜ëŒ€/ë¹„íŒ': '#F44336',
                'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
                'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
                'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
            }
        )
        fig_pie.update_traces(textinfo='percent+label')
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with col2:
        # ë§‰ëŒ€ ì°¨íŠ¸
        fig_bar = px.bar(
            x=type_counts.index,
            y=type_counts.values,
            title="ëŒ“ê¸€ ìœ í˜•ë³„ ê°œìˆ˜",
            color=type_counts.index,
            color_discrete_map={
                'ì°¬ì„±/ì§€ì§€': '#4CAF50',
                'ë°˜ëŒ€/ë¹„íŒ': '#F44336',
                'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
                'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
                'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
            }
        )
        fig_bar.update_layout(showlegend=False)
        st.plotly_chart(fig_bar, use_container_width=True)
    
    # ì‹œê°„ë³„ ê°ì • íë¦„
    hourly_sentiment = analyze_sentiment_trend(comments_df)
    
    if not hourly_sentiment.empty:
        st.subheader("ğŸ“ˆ ì‹œê°„ë³„ ëŒ“ê¸€ ìœ í˜• ë³€í™”")
        
        fig_timeline = go.Figure()
        
        colors = {
            'ì°¬ì„±/ì§€ì§€': '#4CAF50',
            'ë°˜ëŒ€/ë¹„íŒ': '#F44336',
            'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
            'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
            'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
        }
        
        for col in hourly_sentiment.columns:
            fig_timeline.add_trace(go.Scatter(
                x=hourly_sentiment.index,
                y=hourly_sentiment[col],
                mode='lines+markers',
                name=col,
                line=dict(color=colors.get(col, '#000000'))
            ))
        
        fig_timeline.update_layout(
            title="ì‹œê°„ë³„ ëŒ“ê¸€ ìœ í˜• ë³€í™”",
            xaxis_title="ì‹œê°„",
            yaxis_title="ëŒ“ê¸€ ìˆ˜",
            hovermode='x unified'
        )
        
        st.plotly_chart(fig_timeline, use_container_width=True)
    
    # ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„
    st.subheader("ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ì „ì²´ í‚¤ì›Œë“œ
        all_text = " ".join(comments_df['text'].astype(str))
        # HTML íƒœê·¸ ì œê±°
        all_text = re.sub(r'<[^>]+>', '', all_text)
        words = re.findall(r'[ê°€-í£a-zA-Z]+', all_text.lower())
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¥
        stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì„œ', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤',
            'í•˜ë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì•„ë‹ˆë‹¤', 'ë³´ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤', 'ì¢€', 'ë”', 'ë˜', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì •ë§',
            'br', 'nbsp', 'gt', 'lt', 'amp', 'quot', 'div', 'span', 'img', 'href', 'http', 'https', 'www',
            'ê·¸ëƒ¥', 'ë§‰', 'í•œí…Œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•˜ê³ ', 'ì´ê³ ', 'ë‘', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
            'ì•ˆ', 'ëª»', 'ì˜', 'ì¢€', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì™„ì „', 'ì—„ì²­', 'ë˜ê²Œ', 'ê²ë‚˜', 'ê°œ', 'ì¡´',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
        }
        
        words = [w for w in words if len(w) > 1 and w not in stop_words and not w.isdigit()]
        
        word_counts = Counter(words).most_common(15)
        
        if word_counts:
            word_df = pd.DataFrame(word_counts, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
            fig_words = px.bar(
                word_df, 
                x='ë¹ˆë„', 
                y='í‚¤ì›Œë“œ',
                orientation='h',
                title="ì „ì²´ ì£¼ìš” í‚¤ì›Œë“œ Top 15"
            )
            fig_words.update_layout(yaxis={'categoryorder':'total ascending'})
            st.plotly_chart(fig_words, use_container_width=True)
    
    with col2:
        # ë¶€ì •ì  ëŒ“ê¸€ì˜ í‚¤ì›Œë“œ
        negative_comments = comments_df[comments_df['type'].isin(['ë°˜ëŒ€/ë¹„íŒ', 'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„'])]
        
        if len(negative_comments) > 0:
            negative_text = " ".join(negative_comments['text'].astype(str))
            # HTML íƒœê·¸ ì œê±°
            negative_text = re.sub(r'<[^>]+>', '', negative_text)
            negative_words = re.findall(r'[ê°€-í£a-zA-Z]+', negative_text.lower())
            
            # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
            stop_words = {
                'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì„œ', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤',
                'í•˜ë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì•„ë‹ˆë‹¤', 'ë³´ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤', 'ì¢€', 'ë”', 'ë˜', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì •ë§',
                'br', 'nbsp', 'gt', 'lt', 'amp', 'quot', 'div', 'span', 'img', 'href', 'http', 'https', 'www',
                'ê·¸ëƒ¥', 'ë§‰', 'í•œí…Œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•˜ê³ ', 'ì´ê³ ', 'ë‘', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
                'ì•ˆ', 'ëª»', 'ì˜', 'ì¢€', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì™„ì „', 'ì—„ì²­', 'ë˜ê²Œ', 'ê²ë‚˜', 'ê°œ', 'ì¡´',
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
            }
            
            negative_words = [w for w in negative_words if len(w) > 1 and w not in stop_words and not w.isdigit()]
            
            negative_word_counts = Counter(negative_words).most_common(10)
            
            if negative_word_counts:
                neg_word_df = pd.DataFrame(negative_word_counts, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                fig_neg_words = px.bar(
                    neg_word_df, 
                    x='ë¹ˆë„', 
                    y='í‚¤ì›Œë“œ',
                    orientation='h',
                    title="ë¶€ì •ì  ëŒ“ê¸€ ì£¼ìš” í‚¤ì›Œë“œ Top 10",
                    color_discrete_sequence=['#F44336']
                )
                fig_neg_words.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig_neg_words, use_container_width=True)
    
    # ìƒì„¸ ëŒ“ê¸€ í…Œì´ë¸”
    st.subheader("ğŸ’¬ ëŒ“ê¸€ ìƒì„¸ ë¶„ì„")
    
    # í•„í„°ë§ ì˜µì…˜
    filter_type = st.selectbox("ëŒ“ê¸€ ìœ í˜• í•„í„°", ['ì „ì²´'] + list(type_counts.index))
    
    if filter_type != 'ì „ì²´':
        filtered_df = comments_df[comments_df['type'] == filter_type]
    else:
        filtered_df = comments_df
    
    # ëŒ“ê¸€ í‘œì‹œ
    display_df = filtered_df[['text', 'type', 'author', 'like_count', 'published_at']].copy()
    display_df.columns = ['ëŒ“ê¸€ ë‚´ìš©', 'ìœ í˜•', 'ì‘ì„±ì', 'ì¢‹ì•„ìš”', 'ì‘ì„±ì‹œê°„']
    display_df = display_df.sort_values('ì¢‹ì•„ìš”', ascending=False)
    
    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "ëŒ“ê¸€ ë‚´ìš©": st.column_config.TextColumn(width="large"),
            "ìœ í˜•": st.column_config.TextColumn(width="medium"),
            "ì‘ì„±ì‹œê°„": st.column_config.DatetimeColumn(format="YYYY-MM-DD HH:mm")
        }
    )
    
    # í†µê³„ ìš”ì•½
    st.subheader("ğŸ“Š ë¶„ì„ ìš”ì•½")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ê¸ì •ì  ëŒ“ê¸€", f"{type_counts.get('ì°¬ì„±/ì§€ì§€', 0)}ê°œ")
        st.metric("ë¶€ì •ì  ëŒ“ê¸€", f"{type_counts.get('ë°˜ëŒ€/ë¹„íŒ', 0)}ê°œ")
    
    with col2:
        st.metric("ì§ˆë¬¸/ì •ë³´ì„±", f"{type_counts.get('ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±', 0)}ê°œ")
        st.metric("ìœ ë¨¸/ê°íƒ„", f"{type_counts.get('ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„', 0)}ê°œ")
    
    with col3:
        positive_ratio = (type_counts.get('ì°¬ì„±/ì§€ì§€', 0) / total_comments * 100)
        negative_ratio = (type_counts.get('ë°˜ëŒ€/ë¹„íŒ', 0) / total_comments * 100)
        
        st.metric("ê¸ì • ë¹„ìœ¨", f"{positive_ratio:.1f}%")
        st.metric("ë¶€ì • ë¹„ìœ¨", f"{negative_ratio:.1f}%")
    
    # í™œìš© ê°€ì´ë“œ
    with st.expander("ğŸ“– ì‹œìŠ¤í…œ í™œìš© ê°€ì´ë“œ"):
        st.markdown("""
        ### ğŸ¯ ì£¼ìš” í™œìš© ë¶„ì•¼
        
        **1. ì •ì±…/ì‚¬íšŒ ì´ìŠˆ ëª¨ë‹ˆí„°ë§**
        - ğŸ¤– AI ê¸°ë°˜ ì •í™•í•œ ê°ì • ë¶„ì„ìœ¼ë¡œ ì—¬ë¡  íŒŒì•…
        - íŠ¹ì • ìœ í˜•(ë°˜ëŒ€, ë¹„ê¼¼ ë“±) ê¸‰ì¦ ì‹œ ì •ì±… ë‹´ë‹¹ìê°€ ë¹ ë¥´ê²Œ ëŒ€ì‘
        - ì—¬ë¡ ì˜ ë³€í™” íë¦„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì•…
        
        **2. ì‚¬íšŒì  ìœ„í—˜ ì¡°ê¸° ê²½ë³´**
        - AI ëª¨ë¸ì˜ ë†’ì€ ì •í™•ë„ë¡œ ìœ„í—˜ ì‹ í˜¸ ì •ë°€ ê°ì§€
        - í˜ì˜¤/ë¶„ë…¸/ë¹„ê¼¼ ëŒ“ê¸€ì´ ì¼ì • ë¹„ìœ¨ ì´ìƒì´ë©´ 'ìœ„í—˜ ì‹ í˜¸' ìë™ ê°ì§€
        - ì‚¬íšŒì  ê°ˆë“±ì´ë‚˜ ë…¼ë€ì˜ ì¡°ê¸° ë°œê²¬
        
        **3. í‚¤ì›Œë“œ/í† í”½ ë¶„ì„**
        - ìµœê·¼ ìŸì ê³¼ ë…¼ë€ì˜ íë¦„ì„ í•œëˆˆì— íŒŒì•…
        - ì£¼ìš” ê´€ì‹¬ì‚¬ì™€ ë¶ˆë§Œì‚¬í•­ ì‹ë³„
        
        **4. ë§ˆì¼€íŒ…/PR ì „ëµ ìˆ˜ë¦½**
        - ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì‹¤ì œ ë°˜ì‘ ë¶„ì„
        - ê¸ì •ì /ë¶€ì •ì  í”¼ë“œë°±ì˜ êµ¬ì²´ì  ë‚´ìš© íŒŒì•…
        
        ### ğŸ¤– AI ëª¨ë¸ ì¥ì 
        - **ë†’ì€ ì •í™•ë„**: ê¸°ì¡´ í‚¤ì›Œë“œ ë°©ì‹ ëŒ€ë¹„ 90%+ í–¥ìƒ
        - **ë¬¸ë§¥ ì´í•´**: ë‹¨ìˆœ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ë¬¸ë§¥ ì „ì²´ë¥¼ ì´í•´
        - **í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ì–¸ì–´ ëª¨ë¸ë¡œ í•œêµ­ì–´ ëŒ“ê¸€ ì •í™• ë¶„ì„
        - **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ë¹ ë¥¸ ë¶„ì„ ì†ë„ë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥
        
        ### âš ï¸ ìœ„í—˜ë„ ê¸°ì¤€
        - **50% ì´ìƒ**: ì¦‰ì‹œ ëŒ€ì‘ í•„ìš” (ìœ„í—˜)
        - **30-50%**: ì£¼ì˜ ê¹Šì€ ëª¨ë‹ˆí„°ë§ í•„ìš” (ê²½ê³ )
        - **30% ë¯¸ë§Œ**: ì•ˆì •ì  ìƒíƒœ (ì–‘í˜¸)
        """)

if __name__ == "__main__":
    main()