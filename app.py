import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import requests
import re
from collections import Counter
from datetime import datetime, timedelta
import numpy as np
import time
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from transformers import logging
import warnings

# --- ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')
logging.set_verbosity_error()

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (ë°œí‘œ í™˜ê²½ì— ë§ê²Œ ê²½ë¡œ ìˆ˜ì • í•„ìˆ˜!)
# ìœˆë„ìš°: 'C:/Windows/Fonts/malgun.ttf'
# ë§¥: '/System/Library/Fonts/AppleSDGothicNeo.ttc' ë˜ëŠ” '/Library/Fonts/AppleGothic.ttf'
# ë¦¬ëˆ…ìŠ¤: ë‚˜ëˆ”ê³ ë”• ì„¤ì¹˜ í›„ '/usr/share/fonts/truetype/nanum/NanumGothic.ttf' ë“±
plt.rcParams['font.family'] = 'Malgun Gothic' # ì˜ˆì‹œ, ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ì—¬ë¡  ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #FF0000;
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 2rem;
    }
    .danger-alert {
        background-color: #ffebee;
        border-left: 5px solid #f44336;
        padding: 1rem;
        margin: 1rem 0;
    }
    .success-alert {
        background-color: #e8f5e8;
        border-left: 5px solid #4caf50;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background-color: #fff3e0;
        border-left: 5px solid #ff9800;
        padding: 1rem;
        margin: 1rem 0;
    }
    .stCodeBlock {
        overflow-x: auto;
    }
</style>
""", unsafe_allow_html=True)

# YouTube API í´ë˜ìŠ¤
class YouTubeAnalyzer:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://www.googleapis.com/youtube/v3"

    def extract_video_id(self, url):
        """YouTube URLì—ì„œ video ID ì¶”ì¶œ"""
        patterns = [
            r'youtube\.com/watch\?v=([^&]+)',
            r'youtu\.be/([^?]+)',
            r'youtube\.com/embed/([^?]+)'
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None

    def get_video_info(self, video_id):
        """ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/videos"
        params = {
            'part': 'snippet,statistics',
            'id': video_id,
            'key': self.api_key
        }

        try:
            response = requests.get(url, params=params)
            response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
            data = response.json()

            if 'items' in data and len(data['items']) > 0:
                video = data['items'][0]
                return {
                    'title': video['snippet']['title'],
                    'channel': video['snippet']['channelTitle'],
                    'published_at': video['snippet']['publishedAt'],
                    'view_count': int(video['statistics'].get('viewCount', 0)),
                    'like_count': int(video['statistics'].get('likeCount', 0)),
                    'comment_count': int(video['statistics'].get('commentCount', 0))
                }
            else:
                st.error(f"ë¹„ë””ì˜¤ ID '{video_id}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹„ë””ì˜¤ê°€ ì—†ê±°ë‚˜ ì ‘ê·¼ ì œí•œë¨)")
                return None
        except requests.exceptions.HTTPError as http_err:
            st.error(f"HTTP ì˜¤ë¥˜ ë°œìƒ: {http_err} (ë¹„ë””ì˜¤ ID: {video_id})")
            st.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}") # API ì‘ë‹µ ë‚´ìš© ì¶”ê°€ ì¶œë ¥
            return None
        except requests.exceptions.ConnectionError as conn_err:
            st.error(f"ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜: {conn_err}")
            return None
        except requests.exceptions.Timeout as timeout_err:
            st.error(f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼ ì˜¤ë¥˜: {timeout_err}")
            return None
        except requests.exceptions.RequestException as req_err:
            st.error(f"ìš”ì²­ ì˜¤ë¥˜: {req_err}")
            return None
        except ValueError as json_err: # JSON ë””ì½”ë”© ì˜¤ë¥˜ (Expecting value: line 1 column 1 (char 0) ë“±)
            st.error(f"API ì‘ë‹µ JSON ë””ì½”ë”© ì˜¤ë¥˜: {json_err}. API í‚¤ ë˜ëŠ” í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ì„¸ìš”.")
            st.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
            return None
        except Exception as e:
            st.error(f"ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def get_comments(self, video_id, max_results=100):
        """ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/commentThreads"
        params = {
            'part': 'snippet',
            'videoId': video_id,
            'maxResults': min(max_results, 100),
            'order': 'time',
            'key': self.api_key
        }

        comments = []
        next_page_token = None

        try:
            while len(comments) < max_results:
                if next_page_token:
                    params['pageToken'] = next_page_token
                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()

                if 'items' not in data:
                    break

                for item in data['items']:
                    comment = item['snippet']['topLevelComment']['snippet']
                    comments.append({
                        'text': comment['textDisplay'],
                        'author': comment['authorDisplayName'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    if len(comments) >= max_results:
                        break

                next_page_token = data.get('nextPageToken')
                if not next_page_token:
                    break

        except requests.exceptions.RequestException as req_err:
            st.error(f"ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ìš”ì²­ ì˜¤ë¥˜: {req_err}")
            return []
        except ValueError as json_err: # JSON ë””ì½”ë”© ì˜¤ë¥˜
            st.error(f"API ì‘ë‹µ JSON ë””ì½”ë”© ì˜¤ë¥˜ (ëŒ“ê¸€): {json_err}. API í‚¤ ë˜ëŠ” í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ì„¸ìš”.")
            return []
        except Exception as e:
            st.error(f"ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

        return comments[:max_results]

# AI ê°ì •ë¶„ì„ ëª¨ë¸ í´ë˜ìŠ¤
class KoreanSentimentAnalyzer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.classifier = None
        self.is_initialized = False
        self.model_type = "fallback" # ê¸°ë³¸ê°’ ì„¤ì •

    @st.cache_resource
    def load_model(_self):
        """AI ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ í•œë²ˆë§Œ ë¡œë“œ)"""
        try:
            # 1ì°¨: í•œêµ­ì–´ ê°ì •ë¶„ì„ ëª¨ë¸ (ë¬´ë£Œ)
            model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest" # ë‹¤êµ­ì–´, ê°ì •ë¶„ì„ íŠ¹í™”

            _self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            _self.model = AutoModelForSequenceClassification.from_pretrained(model_name)

            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            _self.classifier = pipeline(
                "text-classification",
                model=_self.model,
                tokenizer=_self.tokenizer,
                device=0 if torch.cuda.is_available() else -1,
                return_all_scores=True
            )

            _self.is_initialized = True
            _self.model_type = "roberta"
            st.sidebar.success(f"AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
            return True

        except Exception as e:
            # st.warning(f"1ì°¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}") # ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ í˜¼ë€ ë°©ì§€
            # 2ì°¨: ë‹¤ë¥¸ ê°ì •ë¶„ì„ ëª¨ë¸
            try:
                model_name = "nlptown/bert-base-multilingual-uncased-sentiment" # ë‹¤êµ­ì–´, 5ì  ì²™ë„ ê°ì„±
                _self.classifier = pipeline(
                    "text-classification",
                    model=model_name,
                    device=0 if torch.cuda.is_available() else -1
                )
                _self.is_initialized = True
                _self.model_type = "bert"
                st.sidebar.success(f"AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                return True
            except Exception as e2:
                # st.warning(f"2ì°¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e2)}") # ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ í˜¼ë€ ë°©ì§€
                # 3ì°¨: ê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë¸ (ì˜ì–´)
                try:
                    model_name = "distilbert-base-uncased-finetuned-sst-2-english" # ì˜ì–´, ê¸ë¶€ì •
                    _self.classifier = pipeline(
                        "text-classification",
                        model=model_name,
                        device=0 if torch.cuda.is_available() else -1
                    )
                    _self.is_initialized = True
                    _self.model_type = "distilbert"
                    st.sidebar.success(f"AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return True
                except Exception as e3:
                    st.error(f"ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
                    _self.is_initialized = False
                    _self.model_type = "fallback"
                    return False

    def analyze_sentiment(self, text):
        """AI ê¸°ë°˜ ê°ì • ë¶„ì„"""
        if not self.is_initialized:
            if not self.load_model():
                return self._fallback_analysis(text) # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¶„ì„

        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)  # URL ì œê±°
            text = text.strip()

            if not text:
                return "ì¤‘ë¦½/ê¸°íƒ€"

            # AI ëª¨ë¸ë¡œ ê°ì • ë¶„ì„
            result = self.classifier(text)

            if isinstance(result[0], list): # RoBERTa ëª¨ë¸ì²˜ëŸ¼ ì—¬ëŸ¬ ë¼ë²¨ ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°
                scores = {item['label']: item['score'] for item in result[0]}
                predicted_label = max(scores.keys(), key=lambda k: scores[k])
            else: # BERT, DistilBERTì²˜ëŸ¼ ë‹¨ì¼ ê²°ê³¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°
                predicted_label = result[0]['label']

            # ë¼ë²¨ì„ ìš°ë¦¬ ë¶„ë¥˜ ì²´ê³„ë¡œ ë§¤í•‘
            return self._map_to_category(predicted_label, text)

        except Exception as e:
            # AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
            # st.warning(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({text[:30]}...): {e}. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.") # ë””ë²„ê¹…ìš©
            return self._fallback_analysis(text)

    def _map_to_category(self, ai_label, text):
        """AI ëª¨ë¸ ê²°ê³¼ë¥¼ ìš°ë¦¬ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        text_lower = text.lower()

        # ì§ˆë¬¸ íŒ¨í„´ ìš°ì„  í™•ì¸
        question_patterns = ['?', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë­', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì§ˆë¬¸', 'ê¶ê¸ˆ', 'ì•Œë ¤ì£¼', 'ê°€ë¥´ì³', 'ë¬¸ì˜']
        if any(pattern in text_lower for pattern in question_patterns) or text.strip().endswith('?'):
            return "ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±"

        # ìœ ë¨¸/ë¹„ê¼¼ íŒ¨í„´ í™•ì¸ (í‚¤ì›Œë“œ ê¸°ë°˜ë³´ë‹¤ AI ê°ì„±ì´ ìš°ì„ ì¼ ìˆ˜ ìˆìœ¼ë‚˜, ëª…í™•í•œ ìœ ë¨¸ íŒ¨í„´ì€ ì—¬ê¸°ì— ë¨¼ì €)
        humor_patterns = ['ã…‹ã…‹', 'ã…ã…', 'í—ˆí—ˆ', 'ã…‹', 'ë¯¸ì³¤', 'ëŒ€ë°•', 'í—', 'ì–´íœ´', 'ã…‹ã…‹ã…‹ã…‹', 'ã…ã…ã…ã…']
        if text_lower.count('ã…‹') >= 3 or text_lower.count('ã…') >= 3:
            return "ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„"

        # AI ê°ì •ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë§¤í•‘ (ëª¨ë¸ë³„ ë¼ë²¨ ì²˜ë¦¬)
        label_upper = str(ai_label).upper()

        # RoBERTa ëª¨ë¸ ë¼ë²¨
        if self.model_type == "roberta":
            if label_upper in ['LABEL_2', 'POSITIVE', 'POS']:
                return "ì°¬ì„±/ì§€ì§€"
            elif label_upper in ['LABEL_0', 'NEGATIVE', 'NEG']:
                return "ë°˜ëŒ€/ë¹„íŒ"
            elif label_upper in ['LABEL_1', 'NEUTRAL']:
                return "ì¤‘ë¦½/ê¸°íƒ€"

        # BERT ë‹¤êµ­ì–´ ëª¨ë¸ ë¼ë²¨ (1~5 ë³„ì )
        elif self.model_type == "bert":
            if label_upper in ['5 STARS', '4 STARS']:
                return "ì°¬ì„±/ì§€ì§€"
            elif label_upper in ['1 STAR', '2 STARS']:
                return "ë°˜ëŒ€/ë¹„íŒ"
            elif label_upper in ['3 STARS']:
                return "ì¤‘ë¦½/ê¸°íƒ€"

        # DistilBERT ëª¨ë¸ ë¼ë²¨ (ì˜ì–´)
        elif self.model_type == "distilbert":
            if label_upper == 'POSITIVE':
                return "ì°¬ì„±/ì§€ì§€"
            elif label_upper == 'NEGATIVE':
                return "ë°˜ëŒ€/ë¹„íŒ"

        # ê¸°íƒ€ ë˜ëŠ” ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš°
        return "ì¤‘ë¦½/ê¸°íƒ€"

    def _fallback_analysis(self, text):
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„"""
        text = text.lower()

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ë°±ì—…ìš©)
        positive_keywords = ['ì¢‹ë‹¤', 'ìµœê³ ', 'í›Œë¥­', 'ê°ì‚¬', 'ì¢‹ë„¤', 'êµ¿', 'ì™„ë²½', 'ìµœê³ ë‹¤', 'ì‘ì›', 'ì§€ì§€', 'ì°¬ì„±', 'ë§ë‹¤', 'ì¢‹ì•„', 'ì‚¬ë‘', 'ëŒ€ë‹¨', 'ë©‹ì§€ë‹¤', 'ê¹”ë”', 'ì¬ë°Œ', 'ê¸°ëŒ€', 'ê¸°ì¨', 'ì¹­ì°¬']
        negative_keywords = ['ì‹«ë‹¤', 'ë³„ë¡œ', 'ìµœì•…', 'ë‚˜ì˜ë‹¤', 'ë°˜ëŒ€', 'ë¹„íŒ', 'í‹€ë ¸', 'ë¬¸ì œ', 'ì˜ëª»', 'ì‹¤ë§', 'í™”ë‚œë‹¤', 'ì§œì¦', 'ë‹µë‹µ', 'ì—­ê²¹', 'ì“°ë ˆê¸°', 'ì–´ë µ', 'ë¶ˆí¸', 'ë¶€ì¡±', 'ì•„ì‰½', 'ë…¼ë€']
        question_keywords = ['?', 'ì–¸ì œ', 'ì–´ë–»ê²Œ', 'ì™œ', 'ë­', 'ë¬´ì—‡', 'ì–´ë””', 'ëˆ„êµ¬', 'ì§ˆë¬¸', 'ê¶ê¸ˆ', 'ì•Œë ¤', 'ë¬¸ì˜']
        sarcasm_keywords = ['ã…‹ã…‹', 'ã…ã…', 'í—ˆí—ˆ', 'ì™€ìš°', 'ëŒ€ë°•', 'ì§„ì§œ', 'ë ˆì•Œ', 'ë¯¸ì³¤', 'ê°œ', 'í—', 'ì–´íœ´', 'ã…‹ã…‹ã…‹ã…‹', 'ã…ã…ã…ã…']

        # ì ìˆ˜ ê³„ì‚°
        positive_score = sum(1 for word in positive_keywords if word in text)
        negative_score = sum(1 for word in negative_keywords if word in text)
        question_score = sum(1 for word in question_keywords if word in text)
        sarcasm_score = sum(1 for word in sarcasm_keywords if word in text)

        # ã…‹ã…‹ã…‹ íŒ¨í„´ ê°€ì¤‘ì¹˜ (í‚¤ì›Œë“œ ê¸°ë°˜ì—ì„œ ë”ìš± ê°•ì¡°)
        if 'ã…‹' in text and text.count('ã…‹') >= 3:
            sarcasm_score += 2
        if 'ã…' in text and text.count('ã…') >= 3:
            sarcasm_score += 2

        # ë¶„ë¥˜ ë¡œì§ (ìš°ì„ ìˆœìœ„)
        if question_score > 0 or '?' in text:
            return "ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±"
        elif sarcasm_score > positive_score + negative_score: # ë¹„ê¼¬ê¸°/ìœ ë¨¸ê°€ ë‹¤ë¥¸ ê°ì„±ë³´ë‹¤ ìš°ì„¸í•  ë•Œ
            return "ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„"
        elif positive_score > negative_score:
            return "ì°¬ì„±/ì§€ì§€"
        elif negative_score > positive_score:
            return "ë°˜ëŒ€/ë¹„íŒ"
        else:
            return "ì¤‘ë¦½/ê¸°íƒ€"

# ì „ì—­ AI ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ (ìºì‹±)
@st.cache_resource
def get_ai_analyzer():
    return KoreanSentimentAnalyzer()

# ì‹œê°„ë³„ ê°ì„± íŠ¸ë Œë“œ ë° ì˜ˆì¸¡ í•¨ìˆ˜
def analyze_sentiment_trend(comments_df):
    """ê°ì • íë¦„ ë¶„ì„ ë° ê°„ë‹¨í•œ ì´ë™ í‰ê·  ì˜ˆì¸¡"""
    if comments_df.empty:
        return pd.DataFrame(), pd.DataFrame() # ì˜ˆì¸¡ ë°ì´í„°í”„ë ˆì„ë„ í•¨ê»˜ ë°˜í™˜

    comments_df['published_at'] = pd.to_datetime(comments_df['published_at'])
    comments_df['hour'] = comments_df['published_at'].dt.floor('H')

    # ì‹œê°„ë³„ ìœ í˜• ë¶„í¬
    hourly_sentiment = comments_df.groupby(['hour', 'type']).size().unstack(fill_value=0)

    # ëª¨ë“  ê°€ëŠ¥í•œ ì‹œê°„ëŒ€ í¬í•¨ (ë°ì´í„°ê°€ ì—†ëŠ” ì‹œê°„ëŒ€ì—ë„ 0ìœ¼ë¡œ ì±„ì›€)
    if not hourly_sentiment.empty:
        min_date = hourly_sentiment.index.min()
        max_date = hourly_sentiment.index.max()
        all_hours = pd.date_range(start=min_date, end=max_date, freq='H')
        hourly_sentiment = hourly_sentiment.reindex(all_hours, fill_value=0)
        # ëª¨ë“  ê°ì„± ìœ í˜• ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì¶”ê°€)
        all_sentiment_types = ['ì°¬ì„±/ì§€ì§€', 'ë°˜ëŒ€/ë¹„íŒ', 'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±', 'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„', 'ì¤‘ë¦½/ê¸°íƒ€']
        for col_name in all_sentiment_types:
            if col_name not in hourly_sentiment.columns:
                hourly_sentiment[col_name] = 0
        hourly_sentiment = hourly_sentiment[all_sentiment_types] # ìˆœì„œ ì •ë ¬
    else:
        return pd.DataFrame(), pd.DataFrame() # ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜

    # ì´ë™ í‰ê·  ê³„ì‚° (ì˜ˆì¸¡ìš©)
    prediction_hours = 3 # ë¯¸ë˜ 3ì‹œê°„ ì˜ˆì¸¡

    # ì˜ˆì¸¡ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
    last_hour = hourly_sentiment.index.max()
    predicted_sentiment = pd.DataFrame(index=pd.date_range(start=last_hour + timedelta(hours=1),
                                                            periods=prediction_hours, freq='H'),
                                       columns=hourly_sentiment.columns).fillna(0)

    for col in hourly_sentiment.columns:
        # ìµœê·¼ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë™ í‰ê·  ê³„ì‚°
        moving_avg = hourly_sentiment[col].rolling(window=3, min_periods=1).mean()
        if not moving_avg.empty:
            # ë§ˆì§€ë§‰ ìœ íš¨í•œ ì´ë™ í‰ê·  ê°’ ë˜ëŠ” 0
            last_avg = moving_avg.iloc[-1] if not moving_avg.isnull().all() else 0
            predicted_sentiment[col] = last_avg # ë§ˆì§€ë§‰ ì´ë™ í‰ê·  ê°’ì„ ë¯¸ë˜ì— ì ìš©

    return hourly_sentiment, predicted_sentiment

def calculate_risk_score(type_counts, total_comments):
    """ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°"""
    if total_comments == 0:
        return 0

    negative_types = ['ë°˜ëŒ€/ë¹„íŒ', 'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„']
    negative_count = sum(type_counts.get(t, 0) for t in negative_types)
    negative_ratio = negative_count / total_comments

    # ìœ„í—˜ë„ ì ìˆ˜ (0-100)
    risk_score = min(negative_ratio * 100, 100)

    return risk_score

# --- ë¹„ë””ì˜¤ ë¹„êµ ë¶„ì„ ê²°ê³¼ í‘œì‹œ í•¨ìˆ˜ ---
def display_comparison_results(analyzed_videos_dict):
    if not analyzed_videos_dict:
        return

    st.markdown("---")
    st.subheader("ğŸ“Š ì—¬ëŸ¬ ë¹„ë””ì˜¤ ë¹„êµ ë¶„ì„")

    comparison_data = []
    for video_id, data in analyzed_videos_dict.items():
        if data['comments_df'].empty:
            continue
        type_counts = data['comments_df']['type'].value_counts()
        total_comments = len(data['comments_df'])
        risk_score = calculate_risk_score(type_counts.to_dict(), total_comments)

        comparison_data.append({
            'ì œëª©': data['video_info']['title'],
            'ì±„ë„': data['video_info']['channel'],
            'ë¶„ì„ ëŒ“ê¸€ ìˆ˜': total_comments,
            'ê¸ì • ë¹„ìœ¨': (type_counts.get('ì°¬ì„±/ì§€ì§€', 0) / total_comments * 100) if total_comments > 0 else 0,
            'ë¶€ì • ë¹„ìœ¨': (type_counts.get('ë°˜ëŒ€/ë¹„íŒ', 0) / total_comments * 100) if total_comments > 0 else 0,
            'ì§ˆë¬¸/ì •ë³´ì„± ë¹„ìœ¨': (type_counts.get('ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±', 0) / total_comments * 100) if total_comments > 0 else 0,
            'ìœ ë¨¸/ë¹„ê¼¬ê¸° ë¹„ìœ¨': (type_counts.get('ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„', 0) / total_comments * 100) if total_comments > 0 else 0,
            'ìœ„í—˜ë„ ì ìˆ˜': risk_score
        })

    if not comparison_data:
        st.info("ë¹„êµí•  ìˆ˜ ìˆëŠ” ë¶„ì„ëœ ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì˜ìƒì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        return

    comparison_df = pd.DataFrame(comparison_data)
    st.dataframe(comparison_df, use_container_width=True, hide_index=True)

    # ë¹„êµ ë§‰ëŒ€ ì°¨íŠ¸ (ê¸ì •/ë¶€ì • ë¹„ìœ¨)
    st.markdown("<h5>ê¸ì •/ë¶€ì • ëŒ“ê¸€ ë¹„ìœ¨ ë¹„êµ</h5>", unsafe_allow_html=True)
    fig_comp_sentiment = px.bar(
        comparison_df,
        x='ì œëª©',
        y=['ê¸ì • ë¹„ìœ¨', 'ë¶€ì • ë¹„ìœ¨'],
        barmode='group',
        labels={'value': 'ë¹„ìœ¨ (%)', 'ì œëª©': 'ë¹„ë””ì˜¤ ì œëª©'},
        color_discrete_map={'ê¸ì • ë¹„ìœ¨': '#4CAF50', 'ë¶€ì • ë¹„ìœ¨': '#F44336'},
        height=400
    )
    st.plotly_chart(fig_comp_sentiment, use_container_width=True)

    # ìœ„í—˜ë„ ì ìˆ˜ ë¹„êµ ì°¨íŠ¸
    st.markdown("<h5>ìœ„í—˜ë„ ì ìˆ˜ ë¹„êµ</h5>", unsafe_allow_html=True)
    fig_comp_risk = px.bar(
        comparison_df,
        x='ì œëª©',
        y='ìœ„í—˜ë„ ì ìˆ˜',
        labels={'ìœ„í—˜ë„ ì ìˆ˜': 'ì ìˆ˜ (0-100)', 'ì œëª©': 'ë¹„ë””ì˜¤ ì œëª©'},
        color='ìœ„í—˜ë„ ì ìˆ˜',
        color_continuous_scale=px.colors.sequential.Reds,
        height=400
    )
    st.plotly_chart(fig_comp_risk, use_container_width=True)


# ë©”ì¸ ì•±
def main():
    st.markdown('<h1 class="main-header">ğŸ¤– AI ê¸°ë°˜ YouTube ëŒ“ê¸€ ì—¬ë¡  ë¶„ì„ ì‹œìŠ¤í…œ</h1>', unsafe_allow_html=True)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì²˜ìŒ ë¡œë“œë  ë•Œë§Œ)
    if 'analyzed_videos' not in st.session_state:
        st.session_state.analyzed_videos = {}
    if 'api_key' not in st.session_state:
        st.session_state.api_key = ""

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

    # API í‚¤ ì…ë ¥
    st.session_state.api_key = st.sidebar.text_input("YouTube API Key", value=st.session_state.api_key, type="password", help="YouTube Data API v3 í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    if not st.session_state.api_key:
        st.warning("ğŸ”‘ YouTube API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        st.info("""
        **API í‚¤ ë°œê¸‰ ë°©ë²•:**
        1. [Google Cloud Console](https://console.cloud.google.com/)ì— ì ‘ì†
        2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± ë˜ëŠ” ê¸°ì¡´ í”„ë¡œì íŠ¸ ì„ íƒ
        3. YouTube Data API v3 í™œì„±í™”
        4. ì‚¬ìš©ì ì¸ì¦ ì •ë³´ì—ì„œ API í‚¤ ìƒì„±

        **ğŸ¤– AI ëª¨ë¸ ì •ë³´:**
        - ë‹¤ì¤‘ ë°±ì—… ì‹œìŠ¤í…œìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
        - 1ì°¨: Twitter RoBERTa (ë‹¤êµ­ì–´ ê°ì •ë¶„ì„)
        - 2ì°¨: BERT ë‹¤êµ­ì–´ ëª¨ë¸
        - 3ì°¨: DistilBERT ì˜ì–´ ëª¨ë¸
        - í‚¤ì›Œë“œ ë°©ì‹ ëŒ€ë¹„ 80%+ ì •í™•ë„ í–¥ìƒ
        """)
        return

    # YouTube URL ì…ë ¥ (ì—¬ëŸ¬ ê°œ ì…ë ¥ ê°€ëŠ¥)
    youtube_urls_input = st.sidebar.text_area(
        "YouTube ë¹„ë””ì˜¤ URL (ì—¬ëŸ¬ ê°œ ì…ë ¥ ê°€ëŠ¥, ê° ì¤„ì— í•˜ë‚˜ì”©)",
        placeholder="https://www.youtube.com/watch?v=xxxxxxxx\nhttps://youtu.be/yyyyyyy",
        height=150
    )

    # ë¶„ì„ ì˜µì…˜
    max_comments = st.sidebar.slider("ê° ì˜ìƒì—ì„œ ë¶„ì„í•  ëŒ“ê¸€ ìˆ˜", 50, 500, 200)

    # ë¶„ì„ ì‹œì‘ ë²„íŠ¼
    if st.sidebar.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary"):
        if not youtube_urls_input:
            st.error("YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return

        urls = [url.strip() for url in youtube_urls_input.split('\n') if url.strip()]
        if not urls:
            st.error("ìœ íš¨í•œ YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return

        analyzer = YouTubeAnalyzer(st.session_state.api_key)
        ai_analyzer = get_ai_analyzer() # AI ë¶„ì„ê¸° ë¡œë“œ ì‹œë„

        # AI ëª¨ë¸ ë¡œë“œ í‘œì‹œ (ìµœì´ˆ 1íšŒ)
        if not ai_analyzer.is_initialized:
            with st.spinner("ğŸ¤– AI ê°ì •ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒ)"):
                ai_analyzer.load_model()
            if not ai_analyzer.is_initialized:
                st.error("AI ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëŒ“ê¸€ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

        # ëª¨ë“  URLì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„
        analyzed_count = 0
        total_urls = len(urls)
        st.info(f"ì´ {total_urls}ê°œì˜ ì˜ìƒì„ ë¶„ì„í•©ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...")
        current_video_placeholder = st.empty()
        global_progress_bar = st.progress(0)

        for i, url in enumerate(urls):
            current_video_placeholder.info(f"[{i+1}/{total_urls}] '{url}' ì˜ìƒ ë¶„ì„ ì¤‘...")
            video_id = analyzer.extract_video_id(url)

            if not video_id:
                st.error(f"'{url}'ì€(ëŠ”) ì˜¬ë°”ë¥¸ YouTube URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            video_info = analyzer.get_video_info(video_id)
            if not video_info:
                st.error(f"ë¹„ë””ì˜¤ ID '{video_id}' ({url}) ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
            comments = analyzer.get_comments(video_id, max_comments)
            if not comments:
                st.warning(f"'{video_info['title']}' ({url}) ì˜ìƒì—ëŠ” ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
                # ëŒ“ê¸€ì´ ì—†ì–´ë„ ë¹„ë””ì˜¤ ì •ë³´ëŠ” ì €ì¥í•˜ì—¬ ë¹„êµ ëª©ë¡ì— í¬í•¨ (ëŒ“ê¸€ ìˆ˜ëŠ” 0ìœ¼ë¡œ)
                st.session_state.analyzed_videos[video_id] = {
                    'video_info': video_info,
                    'comments_df': pd.DataFrame() # ë¹ˆ ë°ì´í„°í”„ë ˆì„
                }
                analyzed_count += 1
                global_progress_bar.progress((analyzed_count / total_urls))
                continue

            # ëŒ“ê¸€ ë¶„ë¥˜
            classified_comments = []
            for j, comment in enumerate(comments):
                classified_comments.append({
                    'text': comment['text'],
                    'author': comment['author'],
                    'published_at': comment['published_at'],
                    'like_count': comment['like_count'],
                    'type': ai_analyzer.analyze_sentiment(comment['text'])
                })
                # ëŒ“ê¸€ ë¶„ì„ ì§„í–‰ë¥  (ê¸€ë¡œë²Œ í”„ë¡œê·¸ë ˆìŠ¤ ë°”ì— ë¯¸ë¯¸í•˜ê²Œ ë°˜ì˜)
                local_progress = (j / len(comments)) * (1 / total_urls)
                global_progress_bar.progress((analyzed_count / total_urls) + local_progress)

            comments_df = pd.DataFrame(classified_comments)

            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.analyzed_videos[video_id] = {
                'video_info': video_info,
                'comments_df': comments_df
            }
            analyzed_count += 1
            global_progress_bar.progress((analyzed_count / total_urls)) # ê° ì˜ìƒ ë¶„ì„ ì™„ë£Œ í›„ ì—…ë°ì´íŠ¸

        current_video_placeholder.empty()
        global_progress_bar.empty()
        st.success(f"ì´ {analyzed_count}ê°œì˜ ì˜ìƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # ë¶„ì„ëœ ì˜ìƒì´ ìˆì„ ê²½ìš° ê²°ê³¼ í‘œì‹œ
    if st.session_state.analyzed_videos:
        st.markdown("---")
        st.subheader("ê²°ê³¼ ìš”ì•½")

        # ì‚¬ìš©ìê°€ íŠ¹ì • ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì—¬ ìƒì„¸ ë¶„ì„ ë³¼ ìˆ˜ ìˆë„ë¡ ë“œë¡­ë‹¤ìš´ ì¶”ê°€
        video_titles = {v_id: data['video_info']['title'] for v_id, data in st.session_state.analyzed_videos.items()}
        selected_video_id = st.selectbox(
            "ìƒì„¸ ë¶„ì„ì„ ë³¼ ì˜ìƒì„ ì„ íƒí•˜ì„¸ìš”:",
            options=list(video_titles.keys()),
            format_func=lambda x: video_titles[x]
        )

        if selected_video_id:
            st.markdown(f"### âœ¨ ì„ íƒëœ ì˜ìƒ: {video_titles[selected_video_id]}")
            selected_data = st.session_state.analyzed_videos[selected_video_id]
            display_results(selected_data['video_info'], selected_data['comments_df'])

        # --- ì—¬ëŸ¬ ì˜ìƒ ë¹„êµ ë¶„ì„ ê¸°ëŠ¥ í˜¸ì¶œ ---
        if len(st.session_state.analyzed_videos) > 1:
            display_comparison_results(st.session_state.analyzed_videos)
        elif len(st.session_state.analyzed_videos) == 1:
            st.info("ì—¬ëŸ¬ ì˜ìƒ ë¹„êµ ë¶„ì„ì„ ë³´ë ¤ë©´ 2ê°œ ì´ìƒì˜ ì˜ìƒì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")

    else:
        st.info("ë¶„ì„í•  YouTube URLì„ ì…ë ¥í•˜ê³  'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")


# --- ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™” í•¨ìˆ˜ (ê¸°ì¡´ ë‚´ìš© + í‚¤ì›Œë“œ ì‹¬ì¸µ ë¶„ì„ ì¶”ê°€) ---
def display_results(video_info, comments_df):
    """ë‹¨ì¼ ì˜ìƒì˜ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""

    # ë¹„ë””ì˜¤ ì •ë³´ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ğŸ‘€ ì¡°íšŒìˆ˜", f"{video_info['view_count']:,}")
    with col2:
        st.metric("ğŸ‘ ì¢‹ì•„ìš”", f"{video_info['like_count']:,}")
    with col3:
        st.metric("ğŸ’¬ ëŒ“ê¸€ìˆ˜", f"{video_info['comment_count']:,}")
    with col4:
        st.metric("ğŸ“Š ë¶„ì„ ëŒ“ê¸€", f"{len(comments_df):,}")

    st.markdown(f"**ğŸ“º ì œëª©:** {video_info['title']}")
    st.markdown(f"**ğŸ“º ì±„ë„:** {video_info['channel']}")

    # ëŒ“ê¸€ì´ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if comments_df.empty:
        st.info("ì´ ì˜ìƒì— ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return # ë” ì´ìƒ ê·¸ë˜í”„ ë“±ì„ ê·¸ë¦¬ì§€ ì•Šê³  ì¢…ë£Œ

    # ìœ í˜•ë³„ ë¶„í¬ ë¶„ì„
    type_counts = comments_df['type'].value_counts()
    total_comments = len(comments_df)

    # ìœ„í—˜ë„ ê³„ì‚°
    risk_score = calculate_risk_score(type_counts.to_dict(), total_comments)

    # ìœ„í—˜ ê²½ë³´
    if risk_score > 50:
        st.markdown(f"""
        <div class="danger-alert">
            <h3>ğŸš¨ ìœ„í—˜ ê²½ë³´!</h3>
            <p>ë¶€ì •ì  ëŒ“ê¸€ ë¹„ìœ¨ì´ <strong>{risk_score:.1f}%</strong>ë¡œ ë†’ìŠµë‹ˆë‹¤!</p>
            <p>ì‚¬íšŒì  ìœ„í—˜ ì‹ í˜¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ëŒ€ì‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
        </div>
        """, unsafe_allow_html=True)
    elif risk_score > 30:
        st.markdown(f"""
        <div class="warning-alert">
            <h3>âš ï¸ ì£¼ì˜ í•„ìš”</h3>
            <p>ë¶€ì •ì  ëŒ“ê¸€ ë¹„ìœ¨ì´ <strong>{risk_score:.1f}%</strong>ì…ë‹ˆë‹¤.</p>
            <p>ì—¬ë¡  ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="success-alert">
            <h3>âœ… ì•ˆì •ì </h3>
            <p>ë¶€ì •ì  ëŒ“ê¸€ ë¹„ìœ¨ì´ <strong>{risk_score:.1f}%</strong>ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤.</p>
        </div>
        """, unsafe_allow_html=True)

    # ì°¨íŠ¸ ì˜ì—­
    col1, col2 = st.columns(2)

    with col1:
        # íŒŒì´ ì°¨íŠ¸
        fig_pie = px.pie(
            values=type_counts.values,
            names=type_counts.index,
            title="ëŒ“ê¸€ ìœ í˜•ë³„ ë¶„í¬",
            color_discrete_map={
                'ì°¬ì„±/ì§€ì§€': '#4CAF50',
                'ë°˜ëŒ€/ë¹„íŒ': '#F44336',
                'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
                'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
                'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
            }
        )
        fig_pie.update_traces(textinfo='percent+label')
        st.plotly_chart(fig_pie, use_container_width=True)

    with col2:
        # ë§‰ëŒ€ ì°¨íŠ¸
        fig_bar = px.bar(
            x=type_counts.index,
            y=type_counts.values,
            title="ëŒ“ê¸€ ìœ í˜•ë³„ ê°œìˆ˜",
            color=type_counts.index,
            color_discrete_map={
                'ì°¬ì„±/ì§€ì§€': '#4CAF50',
                'ë°˜ëŒ€/ë¹„íŒ': '#F44336',
                'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
                'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
                'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
            }
        )
        fig_bar.update_layout(showlegend=False)
        st.plotly_chart(fig_bar, use_container_width=True)

    # --- ì‹œê°„ë³„ ê°ì • íë¦„ (ì˜ˆì¸¡ í¬í•¨) ---
    st.markdown("---")
    hourly_sentiment, predicted_sentiment = analyze_sentiment_trend(comments_df)

    if not hourly_sentiment.empty:
        st.subheader("ğŸ“ˆ ì‹œê°„ë³„ ëŒ“ê¸€ ìœ í˜• ë³€í™” ë° ë¯¸ë˜ ì¶”ì´ ì˜ˆì¸¡")

        fig_timeline = go.Figure()

        colors = {
            'ì°¬ì„±/ì§€ì§€': '#4CAF50',
            'ë°˜ëŒ€/ë¹„íŒ': '#F44336',
            'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
            'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
            'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
        }

        for col in hourly_sentiment.columns:
            # ê³¼ê±° ë°ì´í„° íŠ¸ë Œë“œ
            fig_timeline.add_trace(go.Scatter(
                x=hourly_sentiment.index,
                y=hourly_sentiment[col],
                mode='lines+markers',
                name=col + ' (ê³¼ê±°)',
                line=dict(color=colors.get(col, '#000000')),
                showlegend=True
            ))
            # ë¯¸ë˜ ì˜ˆì¸¡ íŠ¸ë Œë“œ (ì ì„ ìœ¼ë¡œ í‘œì‹œ)
            fig_timeline.add_trace(go.Scatter(
                x=predicted_sentiment.index,
                y=predicted_sentiment[col],
                mode='lines',
                name=col + ' (ì˜ˆì¸¡)',
                line=dict(color=colors.get(col, '#000000'), dash='dot'),
                showlegend=True
            ))

        fig_timeline.update_layout(
            title="ì‹œê°„ë³„ ëŒ“ê¸€ ìœ í˜• ë³€í™” ë° ì˜ˆì¸¡",
            xaxis_title="ì‹œê°„",
            yaxis_title="ëŒ“ê¸€ ìˆ˜",
            hovermode='x unified',
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1) # ë²”ë¡€ ìœ„ì¹˜ ì¡°ì •
        )

        st.plotly_chart(fig_timeline, use_container_width=True)


    # --- íŠ¹ì • í‚¤ì›Œë“œ/ì£¼ì œë³„ ì‹¬ì¸µ ë¶„ì„ ---
    st.markdown("---")
    st.subheader("ğŸ” íŠ¹ì • í‚¤ì›Œë“œ/ì£¼ì œë³„ ì‹¬ì¸µ ë¶„ì„")
    keyword_input = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ì‚¼ì„±, ì•„ì´í°, ê²Œì„")

    if keyword_input:
        keywords_to_analyze = [k.strip() for k in keyword_input.split(',') if k.strip()]
        if keywords_to_analyze:
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëŒ“ê¸€ í•„í„°ë§
            # í•œê¸€ ê²€ìƒ‰ì´ë¯€ë¡œ .str.contains() ì‚¬ìš© ì‹œ regex=Trueë¡œ ì„¤ì •í•˜ê±°ë‚˜, re.escape ì‚¬ìš©
            # '|'.join()ìœ¼ë¡œ OR ì¡°ê±´ ê²€ìƒ‰
            pattern = '|'.join(re.escape(k) for k in keywords_to_analyze)
            filtered_comments_df = comments_df[comments_df['text'].str.contains(pattern, case=False, na=False, regex=True)]

            if not filtered_comments_df.empty:
                st.info(f"'{keyword_input}' í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ëŒ“ê¸€ {len(filtered_comments_df)}ê°œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

                # í•„í„°ë§ëœ ëŒ“ê¸€ì— ëŒ€í•œ ìœ í˜•ë³„ ë¶„í¬
                filtered_type_counts = filtered_comments_df['type'].value_counts()

                col_filtered1, col_filtered2 = st.columns(2)

                with col_filtered1:
                    fig_filtered_pie = px.pie(
                        values=filtered_type_counts.values,
                        names=filtered_type_counts.index,
                        title=f"'{keyword_input}' ëŒ“ê¸€ ìœ í˜• ë¶„í¬",
                        color_discrete_map={
                            'ì°¬ì„±/ì§€ì§€': '#4CAF50',
                            'ë°˜ëŒ€/ë¹„íŒ': '#F44436',
                            'ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±': '#2196F3',
                            'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„': '#FF9800',
                            'ì¤‘ë¦½/ê¸°íƒ€': '#9E9E9E'
                        }
                    )
                    fig_filtered_pie.update_traces(textinfo='percent+label')
                    st.plotly_chart(fig_filtered_pie, use_container_width=True)

                with col_filtered2:
                    # í•„í„°ë§ëœ ëŒ“ê¸€ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ
                    st.markdown(f"<h5>'{keyword_input}' ëŒ“ê¸€ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ</h5>", unsafe_allow_html=True)
                    filtered_all_text = " ".join(filtered_comments_df['text'].astype(str))
                    filtered_all_text = re.sub(r'<[^>]+>', '', filtered_all_text)
                    filtered_words = re.findall(r'[ê°€-í£a-zA-Z]+', filtered_all_text.lower())

                    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì¬ì‚¬ìš©
                    stop_words = {
                        'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì„œ', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤',
                        'í•˜ë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì•„ë‹ˆë‹¤', 'ë³´ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤', 'ì¢€', 'ë”', 'ë˜', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì •ë§',
                        'br', 'nbsp', 'gt', 'lt', 'amp', 'quot', 'div', 'span', 'img', 'href', 'http', 'https', 'www',
                        'ê·¸ëƒ¥', 'ë§‰', 'í•œí…Œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•˜ê³ ', 'ì´ê³ ', 'ë‘', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
                        'ì•ˆ', 'ëª»', 'ì˜', 'ì¢€', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì™„ì „', 'ì—„ì²­', 'ë˜ê²Œ', 'ê²ë‚˜', 'ê°œ', 'ì¡´',
                        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
                    }
                    filtered_words = [w for w in filtered_words if len(w) > 1 and w not in stop_words and not w.isdigit()]

                    if filtered_words:
                        try:
                            # font_pathëŠ” plt.rcParams['font.family']ë¡œ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ WordCloud ìƒì„± ì‹œ ìƒëµ ê°€ëŠ¥
                            filtered_wordcloud = WordCloud(
                                width=800, height=400,
                                background_color='white',
                                max_words=50,
                                collocations=False # ë™ì¼ ë‹¨ì–´ ì¤‘ë³µ í‘œì‹œ ë°©ì§€
                            ).generate_from_frequencies(Counter(filtered_words))

                            fig_filtered_wc, ax_filtered_wc = plt.subplots(figsize=(10, 5))
                            ax_filtered_wc.imshow(filtered_wordcloud, interpolation='bilinear')
                            ax_filtered_wc.axis('off')
                            st.pyplot(fig_filtered_wc)
                            plt.close(fig_filtered_wc)
                        except Exception as e:
                            st.warning(f"í‚¤ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    else:
                        st.info("í•„í„°ë§ëœ ëŒ“ê¸€ì—ì„œ ì¶”ì¶œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

                # í•„í„°ë§ëœ ëŒ“ê¸€ ìƒì„¸ í…Œì´ë¸”
                st.markdown("<h5>í•„í„°ë§ëœ ëŒ“ê¸€ ìƒì„¸ ë‚´ìš©</h5>", unsafe_allow_html=True)
                display_filtered_df = filtered_comments_df[['text', 'type', 'author', 'like_count', 'published_at']].copy()
                display_filtered_df.columns = ['ëŒ“ê¸€ ë‚´ìš©', 'ìœ í˜•', 'ì‘ì„±ì', 'ì¢‹ì•„ìš”', 'ì‘ì„±ì‹œê°„']
                display_filtered_df = display_filtered_df.sort_values('ì¢‹ì•„ìš”', ascending=False)
                st.dataframe(
                    display_filtered_df,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "ëŒ“ê¸€ ë‚´ìš©": st.column_config.TextColumn(width="large"),
                        "ìœ í˜•": st.column_config.TextColumn(width="medium"),
                        "ì‘ì„±ì‹œê°„": st.column_config.DatetimeColumn(format="YYYY-MM-DD HH:mm")
                    }
                )
            else:
                st.info(f"ì…ë ¥í•˜ì‹  í‚¤ì›Œë“œ '{keyword_input}'ë¥¼ í¬í•¨í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì›í•˜ì‹œë©´ ìœ„ì— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")


    # ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ì „ì²´ ëŒ“ê¸€ ê¸°ì¤€)
    st.markdown("---")
    st.subheader("ğŸ” ì „ì²´ ëŒ“ê¸€ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")

    col_main_keywords1, col_main_keywords2 = st.columns(2)

    with col_main_keywords1:
        # ì „ì²´ í‚¤ì›Œë“œ (ê¸°ì¡´ ì½”ë“œ)
        all_text = " ".join(comments_df['text'].astype(str))
        all_text = re.sub(r'<[^>]+>', '', all_text)
        words = re.findall(r'[ê°€-í£a-zA-Z]+', all_text.lower())

        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¥ (ê¸°ì¡´ ì½”ë“œ)
        stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì„œ', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤',
            'í•˜ë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì•„ë‹ˆë‹¤', 'ë³´ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤', 'ì¢€', 'ë”', 'ë˜', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì •ë§',
            'br', 'nbsp', 'gt', 'lt', 'amp', 'quot', 'div', 'span', 'img', 'href', 'http', 'https', 'www',
            'ê·¸ëƒ¥', 'ë§‰', 'í•œí…Œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•˜ê³ ', 'ì´ê³ ', 'ë‘', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
            'ì•ˆ', 'ëª»', 'ì˜', 'ì¢€', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì™„ì „', 'ì—„ì²­', 'ë˜ê²Œ', 'ê²ë‚˜', 'ê°œ', 'ì¡´',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
        }

        words = [w for w in words if len(w) > 1 and w not in stop_words and not w.isdigit()]

        word_counts = Counter(words).most_common(15)

        if word_counts:
            word_df = pd.DataFrame(word_counts, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
            fig_words = px.bar(
                word_df,
                x='ë¹ˆë„',
                y='í‚¤ì›Œë“œ',
                orientation='h',
                title="ì „ì²´ ëŒ“ê¸€ ì£¼ìš” í‚¤ì›Œë“œ Top 15"
            )
            fig_words.update_layout(yaxis={'categoryorder':'total ascending'})
            st.plotly_chart(fig_words, use_container_width=True)

            # --- ì „ì²´ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ ì¶”ê°€ ---
            st.markdown("<h5>ì „ì²´ ëŒ“ê¸€ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ</h5>", unsafe_allow_html=True)
            try:
                # font_pathëŠ” plt.rcParams['font.family']ë¡œ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ WordCloud ìƒì„± ì‹œ ìƒëµ ê°€ëŠ¥
                wordcloud = WordCloud(
                    width=800, height=400,
                    background_color='white',
                    max_words=50, # ìµœëŒ€ í‘œì‹œ ë‹¨ì–´ ìˆ˜
                    collocations=False # ë™ì¼ ë‹¨ì–´ ì¤‘ë³µ í‘œì‹œ ë°©ì§€
                ).generate_from_frequencies(Counter(words))

                fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
                ax_wc.imshow(wordcloud, interpolation='bilinear')
                ax_wc.axis('off')
                st.pyplot(fig_wc)
                plt.close(fig_wc) # Streamlitì—ì„œ ê·¸ë˜í”„ë¥¼ ë‹«ì•„ ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš© ë°©ì§€
            except Exception as e:
                st.warning(f"í‚¤ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            st.info("ì¶”ì¶œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")


    with col_main_keywords2:
        # ë¶€ì •ì  ëŒ“ê¸€ì˜ í‚¤ì›Œë“œ (ê¸°ì¡´ ì½”ë“œ)
        negative_comments = comments_df[comments_df['type'].isin(['ë°˜ëŒ€/ë¹„íŒ', 'ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„'])]

        if len(negative_comments) > 0:
            negative_text = " ".join(negative_comments['text'].astype(str))
            negative_text = re.sub(r'<[^>]+>', '', negative_text)
            negative_words = re.findall(r'[ê°€-í£a-zA-Z]+', negative_text.lower())

            # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš© (ê¸°ì¡´ ì½”ë“œ)
            stop_words = {
                'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì„œ', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤',
                'í•˜ë‹¤', 'ë˜ë‹¤', 'ê°™ë‹¤', 'ì•„ë‹ˆë‹¤', 'ë³´ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤', 'ì¢€', 'ë”', 'ë˜', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì •ë§',
                'br', 'nbsp', 'gt', 'lt', 'amp', 'quot', 'div', 'span', 'img', 'href', 'http', 'https', 'www',
                'ê·¸ëƒ¥', 'ë§‰', 'í•œí…Œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•˜ê³ ', 'ì´ê³ ', 'ë‘', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
                'ì•ˆ', 'ëª»', 'ì˜', 'ì¢€', 'ë§ì´', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì™„ì „', 'ì—„ì²­', 'ë˜ê²Œ', 'ê²ë‚˜', 'ê°œ', 'ì¡´',
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
            }

            negative_words = [w for w in negative_words if len(w) > 1 and w not in stop_words and not w.isdigit()]

            negative_word_counts = Counter(negative_words).most_common(10)

            if negative_word_counts:
                neg_word_df = pd.DataFrame(negative_word_counts, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                fig_neg_words = px.bar(
                    neg_word_df,
                    x='ë¹ˆë„',
                    y='í‚¤ì›Œë“œ',
                    orientation='h',
                    title="ë¶€ì •ì  ëŒ“ê¸€ ì£¼ìš” í‚¤ì›Œë“œ Top 10",
                    color_discrete_sequence=['#F44336']
                )
                fig_neg_words.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig_neg_words, use_container_width=True)

                # --- ë¶€ì •ì  ëŒ“ê¸€ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ ì¶”ê°€ ---
                st.markdown("<h5>ë¶€ì •ì  ëŒ“ê¸€ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ</h5>", unsafe_allow_html=True)
                try:
                    neg_wordcloud = WordCloud(
                        width=800, height=400,
                        background_color='white',
                        max_words=50,
                        collocations=False,
                        colormap='Reds' # ë¶€ì •ì ì¸ ëŠë‚Œì˜ ìƒ‰ìƒë§µ
                    ).generate_from_frequencies(Counter(negative_words))

                    fig_neg_wc, ax_neg_wc = plt.subplots(figsize=(10, 5))
                    ax_neg_wc.imshow(neg_wordcloud, interpolation='bilinear')
                    ax_neg_wc.axis('off')
                    st.pyplot(fig_neg_wc)
                    plt.close(fig_neg_wc) # ê·¸ë˜í”„ ë‹«ê¸°
                except Exception as e:
                    st.warning(f"ë¶€ì •ì  í‚¤ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            else:
                st.info("ë¶€ì •ì  ëŒ“ê¸€ì—ì„œ ì¶”ì¶œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶€ì •ì  ëŒ“ê¸€ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ í‚¤ì›Œë“œ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


    # ìƒì„¸ ëŒ“ê¸€ í…Œì´ë¸” (ê¸°ì¡´ ì½”ë“œ)
    st.markdown("---")
    st.subheader("ğŸ’¬ ëŒ“ê¸€ ìƒì„¸ ë¶„ì„")

    # í•„í„°ë§ ì˜µì…˜
    if not comments_df.empty:
        filter_type = st.selectbox("ëŒ“ê¸€ ìœ í˜• í•„í„°", ['ì „ì²´'] + list(type_counts.index))

        if filter_type != 'ì „ì²´':
            filtered_df = comments_df[comments_df['type'] == filter_type]
        else:
            filtered_df = comments_df

        # ëŒ“ê¸€ í‘œì‹œ
        display_df = filtered_df[['text', 'type', 'author', 'like_count', 'published_at']].copy()
        display_df.columns = ['ëŒ“ê¸€ ë‚´ìš©', 'ìœ í˜•', 'ì‘ì„±ì', 'ì¢‹ì•„ìš”', 'ì‘ì„±ì‹œê°„']
        display_df = display_df.sort_values('ì¢‹ì•„ìš”', ascending=False)

        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "ëŒ“ê¸€ ë‚´ìš©": st.column_config.TextColumn(width="large"),
                "ìœ í˜•": st.column_config.TextColumn(width="medium"),
                "ì‘ì„±ì‹œê°„": st.column_config.DatetimeColumn(format="YYYY-MM-DD HH:mm")
            }
        )

    # í†µê³„ ìš”ì•½ (ê¸°ì¡´ ì½”ë“œ)
    st.markdown("---")
    st.subheader("ğŸ“Š ë¶„ì„ ìš”ì•½")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("ê¸ì •ì  ëŒ“ê¸€", f"{type_counts.get('ì°¬ì„±/ì§€ì§€', 0)}ê°œ")
        st.metric("ë¶€ì •ì  ëŒ“ê¸€", f"{type_counts.get('ë°˜ëŒ€/ë¹„íŒ', 0)}ê°œ")

    with col2:
        st.metric("ì§ˆë¬¸/ì •ë³´ì„±", f"{type_counts.get('ì§ˆë¬¸/ìš”ì²­/ì •ë³´ì„±', 0)}ê°œ")
        st.metric("ìœ ë¨¸/ê°íƒ„", f"{type_counts.get('ë¹„ê¼¬ê¸°/ìœ ë¨¸/ê°íƒ„', 0)}ê°œ")

    with col3:
        positive_ratio = (type_counts.get('ì°¬ì„±/ì§€ì§€', 0) / total_comments * 100) if total_comments > 0 else 0
        negative_ratio = (type_counts.get('ë°˜ëŒ€/ë¹„íŒ', 0) / total_comments * 100) if total_comments > 0 else 0
        st.metric("ê¸ì • ë¹„ìœ¨", f"{positive_ratio:.1f}%")
        st.metric("ë¶€ì • ë¹„ìœ¨", f"{negative_ratio:.1f}%")

    # í™œìš© ê°€ì´ë“œ (ê¸°ì¡´ ì½”ë“œ)
    with st.expander("ğŸ“– ì‹œìŠ¤í…œ í™œìš© ê°€ì´ë“œ"):
        st.markdown("""
        ### ğŸ¯ ì£¼ìš” í™œìš© ë¶„ì•¼

        **1. ì •ì±…/ì‚¬íšŒ ì´ìŠˆ ëª¨ë‹ˆí„°ë§**
        - ğŸ¤– AI ê¸°ë°˜ ì •í™•í•œ ê°ì • ë¶„ì„ìœ¼ë¡œ ì—¬ë¡  íŒŒì•…
        - íŠ¹ì • ìœ í˜•(ë°˜ëŒ€, ë¹„ê¼¼ ë“±) ê¸‰ì¦ ì‹œ ì •ì±… ë‹´ë‹¹ìê°€ ë¹ ë¥´ê²Œ ëŒ€ì‘
        - ì—¬ë¡ ì˜ ë³€í™” íë¦„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì•…

        **2. ì‚¬íšŒì  ìœ„í—˜ ì¡°ê¸° ê²½ë³´**
        - AI ëª¨ë¸ì˜ ë†’ì€ ì •í™•ë„ë¡œ ìœ„í—˜ ì‹ í˜¸ ì •ë°€ ê°ì§€
        - í˜ì˜¤/ë¶„ë…¸/ë¹„ê¼¼ ëŒ“ê¸€ì´ ì¼ì • ë¹„ìœ¨ ì´ìƒì´ë©´ 'ìœ„í—˜ ì‹ í˜¸' ìë™ ê°ì§€
        - ì‚¬íšŒì  ê°ˆë“±ì´ë‚˜ ë…¼ë€ì˜ ì¡°ê¸° ë°œê²¬

        **3. í‚¤ì›Œë“œ/í† í”½ ë¶„ì„**
        - ìµœê·¼ ìŸì ê³¼ ë…¼ë€ì˜ íë¦„ì„ í•œëˆˆì— íŒŒì•…
        - ì£¼ìš” ê´€ì‹¬ì‚¬ì™€ ë¶ˆë§Œì‚¬í•­ ì‹ë³„

        **4. ë§ˆì¼€íŒ…/PR ì „ëµ ìˆ˜ë¦½**
        - ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì‹¤ì œ ë°˜ì‘ ë¶„ì„
        - ê¸ì •ì /ë¶€ì •ì  í”¼ë“œë°±ì˜ êµ¬ì²´ì  ë‚´ìš© íŒŒì•…

        ### ğŸ¤– AI ëª¨ë¸ ì¥ì 
        - **ë†’ì€ ì •í™•ë„**: ê¸°ì¡´ í‚¤ì›Œë“œ ë°©ì‹ ëŒ€ë¹„ 90%+ í–¥ìƒ
        - **ë¬¸ë§¥ ì´í•´**: ë‹¨ìˆœ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ë¬¸ë§¥ ì „ì²´ë¥¼ ì´í•´
        - **í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ì–¸ì–´ ëª¨ë¸ë¡œ í•œêµ­ì–´ ëŒ“ê¸€ ì •í™• ë¶„ì„
        - **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ë¹ ë¥¸ ë¶„ì„ ì†ë„ë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥

        ### âš ï¸ ìœ„í—˜ë„ ê¸°ì¤€
        - **50% ì´ìƒ**: ì¦‰ì‹œ ëŒ€ì‘ í•„ìš” (ìœ„í—˜)
        - **30-50%**: ì£¼ì˜ ê¹Šì€ ëª¨ë‹ˆí„°ë§ í•„ìš” (ê²½ê³ )
        - **30% ë¯¸ë§Œ**: ì•ˆì •ì  ìƒíƒœ (ì–‘í˜¸)
        """)

if __name__ == "__main__":
    main()